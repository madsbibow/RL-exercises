{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ValueIterStuff.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhftSvfB2dH+vu3IYqsf4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madsbibow/RL-exercises/blob/master/ValueIterStuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K-jsysoUpio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2cHqFaGWph8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MDP:\n",
        "    '''A simple MDP class.  It includes the following members'''\n",
        "\n",
        "    def __init__(self,T,R,discount):\n",
        "        '''Constructor for the MDP class\n",
        "\n",
        "        Inputs:\n",
        "        T -- Transition function: |A| x |S| x |S'| array\n",
        "        R -- Reward function: |A| x |S| array\n",
        "        discount -- discount factor: scalar in [0,1)\n",
        "\n",
        "        The constructor verifies that the inputs are valid and sets\n",
        "        corresponding variables in a MDP object'''\n",
        "\n",
        "        assert T.ndim == 3, \"Invalid transition function: it should have 3 dimensions\"\n",
        "        self.nActions = T.shape[0]\n",
        "        self.nStates = T.shape[1]\n",
        "        assert T.shape == (self.nActions,self.nStates,self.nStates), \"Invalid transition function: it has dimensionality \" + repr(T.shape) + \", but it should be (nActions,nStates,nStates)\"\n",
        "        assert (abs(T.sum(2)-1) < 1e-5).all(), \"Invalid transition function: some transition probability does not equal 1\"\n",
        "        self.T = T\n",
        "        assert R.ndim == 2, \"Invalid reward function: it should have 2 dimensions\" \n",
        "        assert R.shape == (self.nActions,self.nStates), \"Invalid reward function: it has dimensionality \" + repr(R.shape) + \", but it should be (nActions,nStates)\"\n",
        "        self.R = R\n",
        "        assert 0 <= discount < 1, \"Invalid discount factor: it should be in [0,1)\"\n",
        "        self.discount = discount\n",
        "        self.state_range = range(self.nStates)\n",
        "\n",
        "    def valueIteration(self,initialV,nIterations=np.inf,tolerance=0.01):\n",
        "        '''Value iteration procedure\n",
        "        V <-- max_a R^a + gamma T^a V\n",
        "\n",
        "        Inputs:\n",
        "        initialV -- Initial value function: array of |S| entries\n",
        "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
        "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
        "\n",
        "        Outputs: \n",
        "        V -- Value function: array of |S| entries\n",
        "        iterId -- # of iterations performed: scalar\n",
        "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
        "        \n",
        "        # temporary values to ensure that the code compiles until this\n",
        "        # function is coded\n",
        "        assert initialV.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(initialV.shape,(self.nStates,))   \n",
        "        V = initialV \n",
        "        iterId = 0\n",
        "        epsilon = 0\n",
        "\n",
        "        while iterId <= nIterations:\n",
        "          V_old = V.copy()\n",
        "          V     = (self.R + self.discount*np.dot(self.T, V)).max(0)\n",
        "          \n",
        "          epsilon = np.abs(V - V_old).max()\n",
        "          if epsilon < tolerance:\n",
        "            break\n",
        "          iterId = iterId + 1\n",
        "        \n",
        "        return [V,iterId,epsilon]\n",
        "\n",
        "    def extractPolicy(self,V):\n",
        "        '''Procedure to extract a policy from a value function\n",
        "        pi <-- argmax_a R^a + gamma T^a V\n",
        "\n",
        "        Inputs:\n",
        "        V -- Value function: array of |S| entries\n",
        "\n",
        "        Output:\n",
        "        policy -- Policy: array of |S| entries'''\n",
        "\n",
        "        # temporary values to ensure that the code compiles until this\n",
        "        # function is coded\n",
        "        assert V.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(V.shape,(self.nStates,))\n",
        "        policy = (self.R + self.discount*np.dot(self.T, V)).argmax(0)\n",
        "\n",
        "        return policy \n",
        "\n",
        "    def evaluatePolicy(self,policy):\n",
        "        '''Evaluate a policy by solving a system of linear equations\n",
        "        V^pi = R^pi + gamma T^pi V^pi\n",
        "\n",
        "        Input:\n",
        "        policy -- Policy: array of |S| entries\n",
        "\n",
        "        Ouput:\n",
        "        V -- Value function: array of |S| entries'''\n",
        "\n",
        "        # temporary values to ensure that the code compiles until this\n",
        "        # function is coded\n",
        "\n",
        "        assert policy.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(policy.shape,(self.nStates,))\n",
        "\n",
        "        R_pi = self.R[policy,self.state_range]\n",
        "        T_pi = self.T[policy,self.state_range,:]\n",
        "        V = np.linalg.solve(np.eye(self.nStates)-self.discount*T_pi,R_pi) \n",
        "        \n",
        "        return V\n",
        "        \n",
        "    def policyIteration(self,initialPolicy,nIterations=np.inf):\n",
        "        '''Policy iteration procedure: alternate between policy\n",
        "        evaluation (solve V^pi = R^pi + gamma T^pi V^pi) and policy\n",
        "        improvement (pi <-- argmax_a R^a + gamma T^a V^pi).\n",
        "\n",
        "        Inputs:\n",
        "        initialPolicy -- Initial policy: array of |S| entries\n",
        "        nIterations -- limit on # of iterations: scalar (default: inf)\n",
        "\n",
        "        Outputs: \n",
        "        policy -- Policy: array of |S| entries\n",
        "        V -- Value function: array of |S| entries\n",
        "        iterId -- # of iterations peformed by modified policy iteration: scalar'''\n",
        "\n",
        "        # temporary values to ensure that the code compiles until this\n",
        "        # function is coded\n",
        "        assert initialPolicy.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(initialPolicy.shape,(self.nStates,))\n",
        "\n",
        "        policy = initialPolicy\n",
        "        V = np.zeros(self.nStates)\n",
        "        iterId = 0\n",
        "\n",
        "        while iterId <= nIterations:\n",
        "            policy_old = policy.copy()\n",
        "            V = self.evaluatePolicy(policy)\n",
        "            \n",
        "            policy = self.extractPolicy(V)\n",
        "\n",
        "            iterId = iterId + 1\n",
        "            \n",
        "            if np.array_equal(policy,policy_old):\n",
        "              break\n",
        "\n",
        "        return [policy,V,iterId]\n",
        "            \n",
        "    def evaluatePolicyPartially(self,policy,initialV,nIterations=np.inf,tolerance=0.01):\n",
        "        '''Partial policy evaluation:\n",
        "        Repeat V^pi <-- R^pi + gamma T^pi V^pi\n",
        "\n",
        "        Inputs:\n",
        "        policy -- Policy: array of |S| entries\n",
        "        initialV -- Initial value function: array of |S| entries\n",
        "        nIterations -- limit on the # of iterations: scalar (default: infinity)\n",
        "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
        "\n",
        "        Outputs: \n",
        "        V -- Value function: array of |S| entries\n",
        "        iterId -- # of iterations performed: scalar\n",
        "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
        "\n",
        "        # temporary values to ensure that the code compiles until this\n",
        "        # function is coded\n",
        "        assert policy.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(policy.shape,(self.nStates,))\n",
        "        assert initialV.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(initialV.shape,(self.nStates,))\n",
        "        \n",
        "        V = initialV\n",
        "        iterId = 0\n",
        "\n",
        "        R_pi = self.R[policy,self.state_range]\n",
        "        T_pi = self.T[policy,self.state_range,:]\n",
        "\n",
        "\n",
        "        while iterId <= nIterations:\n",
        "            V_old = V.copy()\n",
        "            V = R_pi + self.discount*np.dot(T_pi,V)\n",
        "            iterId = iterId + 1  \n",
        "            epsilon = np.max(np.abs(V-V_old))\n",
        "            if epsilon < tolerance:\n",
        "              break  \n",
        "\n",
        "        return [V,iterId,epsilon]\n",
        "\n",
        "    def modifiedPolicyIteration(self,initialPolicy,initialV,nEvalIterations=5,nIterations=np.inf,tolerance=0.01):\n",
        "        '''Modified policy iteration procedure: alternate between\n",
        "        partial policy evaluation (repeat a few times V^pi <-- R^pi + gamma T^pi V^pi)\n",
        "        and policy improvement (pi <-- argmax_a R^a + gamma T^a V^pi)\n",
        "\n",
        "        Inputs:\n",
        "        initialPolicy -- Initial policy: array of |S| entries\n",
        "        initialV -- Initial value function: array of |S| entries\n",
        "        nEvalIterations -- limit on # of iterations to be performed in each partial policy evaluation: scalar (default: 5)\n",
        "        nIterations -- limit on # of iterations to be performed in modified policy iteration: scalar (default: inf)\n",
        "        tolerance -- threshold on ||V^n-V^n+1||_inf: scalar (default: 0.01)\n",
        "\n",
        "        Outputs: \n",
        "        policy -- Policy: array of |S| entries\n",
        "        V -- Value function: array of |S| entries\n",
        "        iterId -- # of iterations peformed by modified policy iteration: scalar\n",
        "        epsilon -- ||V^n-V^n+1||_inf: scalar'''\n",
        "\n",
        "        # temporary values to ensure that the code compiles until this\n",
        "        # function is coded\n",
        "        assert initialPolicy.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(initialPolicy.shape,(self.nStates,))\n",
        "        assert initialV.shape == (self.nStates,), \"Invalid initial value: it has dimensionality {}, but is should be {}\".format(initialV.shape,(self.nStates,))        \n",
        "        policy = initialPolicy\n",
        "        V = initialV\n",
        "        iterId = 0\n",
        "        epsilon = 0\n",
        "\n",
        "        while iterId <= nIterations:\n",
        "            V_old = V.copy()\n",
        "            #policy_old = policy.copy()\n",
        "            V, _ , _ = self.evaluatePolicyPartially(policy,V,nEvalIterations,tolerance)\n",
        "            policy = self.extractPolicy(V)\n",
        "            iterId = iterId + 1  \n",
        "            epsilon = np.max(np.abs(V-V_old))\n",
        "            if  epsilon < tolerance:\n",
        "              break\n",
        "\n",
        "\n",
        "        return [policy,V,iterId,epsilon]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPMmwfYTZ15o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Construct simple MDP as described in Lecture 2a Slides 13-14'''\n",
        "# Transition function: |A| x |S| x |S'| array\n",
        "T = np.array([[[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]],[[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]]])\n",
        "# Reward function: |A| x |S| array\n",
        "R = np.array([[0,0,10,10],[0,0,10,10]])\n",
        "# Discount factor: scalar in [0,1)\n",
        "discount = 0.9        \n",
        "# MDP object\n",
        "mdp = MDP(T,R,discount)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNpybCEGaFw8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b98fe74a-0294-4c7e-c858-953dbca76f1e"
      },
      "source": [
        "[V,nIterations,epsilon] = mdp.valueIteration(initialV=np.zeros(mdp.nStates))\n",
        "policy = mdp.extractPolicy(V)\n",
        "\n",
        "print('Iterations {}, epsilon{}'.format(nIterations,epsilon))\n",
        "print('V',V)\n",
        "print('policy',policy)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations 57, epsilon0.009860138819838937\n",
            "V [31.49636306 38.51527513 43.935435   54.1128575 ]\n",
            "policy [0 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zh51-dsmcwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d1457f9-7096-4afb-b270-35bc5942c665"
      },
      "source": [
        "V_CHECK = mdp.evaluatePolicy(policy)\n",
        "print(V_CHECK)\n",
        "V = mdp.evaluatePolicy(np.array([1,0,1,0]))\n",
        "print(V)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[31.58510431 38.60401638 44.02417625 54.20159875]\n",
            "[-0.         -0.         18.18181818 10.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqp4fF3Wp9Js",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a5a9939-fa27-40d3-c8d2-9366014fbe7b"
      },
      "source": [
        "[policy,V,iterId] = mdp.policyIteration(np.array([0,0,0,0]))\n",
        "\n",
        "print(policy,V,iterId)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 1] [31.58510431 38.60401638 44.02417625 54.20159875] 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMpEA6emb09u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d25c8fbc-9430-4f27-a8db-a8d2c0dd48cf"
      },
      "source": [
        "[V,iterId,epsilon] = mdp.evaluatePolicyPartially(np.array([1,0,1,0]),np.array([0,10,0,13]))\n",
        "print(V,iterId,epsilon)\n",
        "\n",
        "[V_CHECK,_,_] = mdp.evaluatePolicyPartially(policy,np.array([0,10,0,13]))\n",
        "print(V_CHECK)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.          0.08727964 18.18181818 10.08727964] 45 0.009697737297875264\n",
            "[31.49784208 38.51675415 43.93691402 54.11433652]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OhFOr4BaD18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d326c72-16e9-400a-a534-b333f727fef6"
      },
      "source": [
        "[policy,V,iterId,epsilon] = mdp.modifiedPolicyIteration(np.array([1,0,1,0]),np.array([0,10,0,13]))\n",
        "\n",
        "print(policy,V,iterId,epsilon)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 1 1] [31.5055624  38.52447447 43.94463435 54.12205685] 12 0.008837989685982706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGZljBxP2M8J",
        "colab_type": "text"
      },
      "source": [
        "All Tests\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpzMBHtq2KXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Test each procedure'''\n",
        "[V,nIterations,epsilon] = mdp.valueIteration(initialV=np.zeros(mdp.nStates))\n",
        "policy = mdp.extractPolicy(V)\n",
        "V = mdp.evaluatePolicy(np.array([1,0,1,0]))\n",
        "[policy,V,iterId] = mdp.policyIteration(np.array([0,0,0,0]))\n",
        "[V,iterId,epsilon] = mdp.evaluatePolicyPartially(np.array([1,0,1,0]),np.array([0,10,0,13]))\n",
        "[policy,V,iterId,epsilon] = mdp.modifiedPolicyIteration(np.array([1,0,1,0]),np.array([0,10,0,13]))"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}